---
title: "st563 final project"
author: "Fangfang Weng"
date: "11/11/2020"
output: word_document
---
```{r}
# Loading data
red<-read.csv("winequality-red.csv",sep=";")
head(red)
dim(red)
# split data into training and test data 
set.seed(1)
train = sample(dim(red)[1], dim(red)[1] / 2)
red.train = red[train, ]
red.test = red[-train, ]
```

```{r}
#KNN
library(class)
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x)) }
##Run nomalization on first 12 coulumns of dataset because they are the predictors
red_norm <- as.data.frame(lapply(red[,-12], nor))
red_norm$quality <-red$quality
## define the test and train data for the KNN
quality.c= ifelse(red$quality >5 , 1, 0)
train.X=red_norm[train,-12]
test.X=red_norm[-train,-12]
train.Y=quality.c[train]
test.Y=quality.c[-train]
train.Y.reg= red_norm$quality[train]
test.Y.reg= red_norm$quality[-train]
#create a list to store the accuracy for each K
accuracy=rep(0,50)
# KNN classification
# for loop to calculate the accuracy for each K
for (i in 1:50){
  set.seed(1)
  knn.pred=knn(train.X,test.X,train.Y,k=i)
  accuracy[i]=mean(knn.pred==test.Y)
}
plot(c(1:50),accuracy,xlab="K",type="o")
# KNN regression
mse=rep(0,50)
# for loop to calculate the accuracy for each K
for (i in 1:50){
  set.seed(1)
  knn.pred.r=FNN::knn.reg(train.X,test.X,train.Y.reg,k=i)$pred
  mse[i]=sqrt(mean((test.Y.reg - knn.pred.r) ^ 2))
}
plot(1:50,mse,xlab="K",type="o")
```
```{r}
#LDA
library(MASS)
new=red
new$quality=quality.c
lda.fit = lda(quality ~ ., data = new, subset = train)
lda.pred = predict(lda.fit,red.test)
lda.class=lda.pred$class
table(lda.class,test.Y)
mean(lda.class==test.Y)
#QDA
qda.fit=qda(quality ~ ., data = new, subset = train)
qda.pred = predict(qda.fit,red.test)
qda.class=qda.pred$class
table(qda.class,test.Y)
mean(qda.class==test.Y)
```
```{r}
library(glmnet)
train.mat = model.matrix(quality~., data=red_norm[train,])
test.mat = model.matrix(quality~., data=red_norm[-train,])
grid = 10 ^ seq(4, -2, length=100)
#ridge
mod.ridge = cv.glmnet(train.mat, red[train,"quality"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = mod.ridge$lambda.min
lambda.best
ridge.pred = predict(mod.ridge, newx=test.mat, s=lambda.best)
mean((red[-train,"quality"] - ridge.pred)^2)
mod.ridge = glmnet(train.mat, red[train,"quality"],  alpha=0)
predict(mod.ridge, s=lambda.best, type="coefficients")
#lasso
mod.lasso = cv.glmnet(train.mat, red[train,"quality"], alpha=1, lambda=grid, thresh=1e-12)
lambda.best = mod.lasso$lambda.min
lambda.best
lasso.pred = predict(mod.lasso, newx=test.mat, s=lambda.best)
mean((red[-train,"quality"] - lasso.pred)^2)
mod.lasso = glmnet(train.mat, red[train,"quality"],  alpha=1)
predict(mod.lasso, s=lambda.best, type="coefficients")
```

